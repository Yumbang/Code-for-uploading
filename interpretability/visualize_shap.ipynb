{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "os.chdir(\"/home/ybang-eai/research/2024/ROMARL/ROMARL\")\n",
    "from TwoStageROProcessEnvironment.env.PressureControlledTwoStageROProcess_simple import TwoStageROProcessEnvironment\n",
    "\n",
    "\n",
    "# agents = ['influent_flowrate']\n",
    "\n",
    "result_path = \"/home/ybang-eai/research/2024/ROMARL/ROMARL/interpretability/result/KernelSHAP/VDN/VDN_data/n_samples-1000\"\n",
    "\n",
    "shap_values = {}\n",
    "observations = {}\n",
    "hidden_states = {}\n",
    "\n",
    "agents = ['influent_flowrate', '1st_stage_pump', '2nd_stage_pump']\n",
    "\n",
    "for agent_name in agents:\n",
    "    with open(f'{result_path}/shap_values_{agent_name}.pkl', 'rb') as f:\n",
    "        shap_values[agent_name] = pickle.load(f)\n",
    "\n",
    "    with open(f'{result_path}/observations_{agent_name}.pkl', 'rb') as f:\n",
    "        observations[agent_name] = pickle.load(f)\n",
    "\n",
    "    with open(f'{result_path}/hidden_states_{agent_name}.pkl', 'rb') as f:\n",
    "        hidden_states[agent_name] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = {\n",
    "    'influent_flowrate': ['feed_flowrate', '1st_permeate_flowrate', '2nd_feed_flowrate', '2nd_permeate_flowrate', '2nd_brine_flowrate', 'temperature'],\n",
    "    '1st_stage_pump':['temperature', 'feed_concentration', 'feed_flowrate', '1st_brine_concentration',\n",
    "                    '1st_brine_flowrate', '1st_permeate_concentration', '1st_permeate_flowrate', '1st_brine_pressure',\n",
    "                    '1st_pressure_applied', '1st_recovery'],\n",
    "    '2nd_stage_pump':['temperature', '2nd_feed_concentration', '2nd_feed_flowrate', '1st_brine_pressure', '2nd_brine_concentration', '2nd_brine_flowrate',\n",
    "                      '2nd_permeate_concentration', '2nd_permeate_flowrate', '2nd_brine_pressure', '2nd_pressure_applied', '1st_recovery']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'silent'\n",
    "save_dir = os.path.join('/home/ybang-eai/research/2024/ROMARL/ROMARL/interpretability', \"VDN\", datetime.now().strftime(\"%y.%m.%d.%H.%M\"))\n",
    "\n",
    "exp_path = {\n",
    "    \"VDN\":  \"/home/ybang-eai/research/2024/ROMARL/ROMARL/interpretability/VDN/25.03.05.15.11\"\n",
    "}\n",
    "exp_type = [\"HinderOneAll\"]\n",
    "\n",
    "print(\"Environment setup ...\")\n",
    "env = TwoStageROProcessEnvironment(render_mode=render_mode, len_scenario=None, save_dir=save_dir)\n",
    "print(\"Environment setup done.\")\n",
    "\n",
    "# agents = ['influent_flowrate']\n",
    "agents = env.agents\n",
    "n_agents = len(agents)\n",
    "\n",
    "device = 'cuda:0'\n",
    "files_in_path = os.listdir(os.path.join(exp_path[\"VDN\"], exp_type[0]))\n",
    "files_in_path = [f for f in files_in_path if f.endswith(\".pkl\")]\n",
    "\n",
    "\n",
    "data = []\n",
    "for file in files_in_path:\n",
    "    split_elements = file.split('_')\n",
    "    episode = split_elements[0]\n",
    "    agent_hindered = split_elements[1]\n",
    "    feed_concentration = split_elements[2]\n",
    "    step = split_elements[-1].split('.')[0]\n",
    "    parameter_type = '_'.join(split_elements[3:-1])\n",
    "    data.append([episode, agent_hindered, feed_concentration, parameter_type, step, file])\n",
    "\n",
    "df_files = pd.DataFrame(data, columns=[\"episode\", \"agent_hindered\", \"feed_concentration\", \"parameter_type\", \"step\", \"file_path\"])\n",
    "\n",
    "# Strip \"ppm\" and convert to float\n",
    "df_files['feed_concentration'] = df_files['feed_concentration'].str.replace('ppm', '').astype(float)\n",
    "df_files['step'] = df_files['step'].astype(int)\n",
    "\n",
    "df_files = df_files.drop(columns=['episode', 'agent_hindered'])\n",
    "\n",
    "# Group by feed_concentration\n",
    "grouped_df = df_files.groupby('feed_concentration')\n",
    "sorted_grouped_df = grouped_df.apply(lambda x: x.sort_values(by='step')).reset_index(drop=True)\n",
    "df_files_sorted = df_files.sort_values(by=['feed_concentration', 'step']).reset_index(drop=True)\n",
    "feed_concentrations = df_files_sorted['feed_concentration'].unique()\n",
    "parameter_types     = df_files_sorted['parameter_type'].unique()\n",
    "steps               = df_files_sorted['step'].unique()\n",
    "\n",
    "input_by_concentration = {}\n",
    "hidden_by_concentration = {}\n",
    "agent_q_by_concentration = {}\n",
    "for feed_c in feed_concentrations:\n",
    "    input_by_concentration[feed_c] = []\n",
    "    target_df = df_files.loc[(df_files['feed_concentration'] == feed_c) & (df_files['parameter_type'] == \"previous_observations_scaled\")].sort_values(by='step')\n",
    "    target_files = target_df['file_path'].values\n",
    "    for file in target_files:\n",
    "        with open(os.path.join(exp_path[\"VDN\"], exp_type[0], file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            input_by_concentration[feed_c].append(data)\n",
    "            \n",
    "    target_hidden_df = df_files.loc[(df_files['feed_concentration'] == feed_c) & (df_files['parameter_type'] == \"hiddens\")].sort_values(by='step')\n",
    "    target_hidden_files = target_hidden_df['file_path'].values\n",
    "    hidden_by_concentration[feed_c] = []\n",
    "    for file in target_hidden_files:\n",
    "        with open(os.path.join(exp_path[\"VDN\"], exp_type[0], file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            hidden_by_concentration[feed_c].append(data)\n",
    "    target_agent_q_df = df_files.loc[(df_files['feed_concentration'] == feed_c) & (df_files['parameter_type'] == \"agent_qs\")].sort_values(by='step')\n",
    "    target_agent_q_files = target_agent_q_df['file_path'].values\n",
    "    agent_q_by_concentration[feed_c] = []\n",
    "    for file in target_agent_q_files:\n",
    "        with open(os.path.join(exp_path[\"VDN\"], exp_type[0], file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            agent_q_by_concentration[feed_c].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_q_by_agent = {}\n",
    "for agent in agents:\n",
    "    agent_q_by_agent[agent] = []\n",
    "    for feed_c in feed_concentrations:\n",
    "        segment_by_concentration = []\n",
    "        for agent_q in agent_q_by_concentration[feed_c]:\n",
    "            segment_by_concentration.append(agent_q[agent].to(device).float())\n",
    "        agent_q_by_agent[agent].append(segment_by_concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_by_agent = {}\n",
    "for agent in agents:\n",
    "    actions_by_agent[agent] = []\n",
    "    for seq_q in agent_q_by_agent[agent]:\n",
    "        for q in seq_q:\n",
    "            actions_by_agent[agent].append(q.argmax().cpu().numpy())\n",
    "    actions_by_agent[agent] = np.array(actions_by_agent[agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_by_agent[agents[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values['1st_stage_pump'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dfs_action_selected = {}\n",
    "for agent in agents:\n",
    "    shap_summary = (np.mean(shap_values[agent], axis=0))\n",
    "\n",
    "    shap_summary_reduced = shap_summary[:-64]\n",
    "    feature_names_reduced = model_features[agent]\n",
    "    shap_df = pd.DataFrame(shap_summary_reduced, index=feature_names_reduced)\n",
    "    \n",
    "    # Use actions_by_agent[agent] to index the columns of shap_df\n",
    "    shap_df = shap_df.iloc[:, actions_by_agent[agent]]\n",
    "    \n",
    "    \n",
    "    print(f\"Shape of shap_df for {agent}: {shap_df.shape}\")\n",
    "    shap_dfs_action_selected[agent] = shap_df\n",
    "\n",
    "\n",
    "for agent in agents:\n",
    "    mean_shap_action_selected = shap_dfs_action_selected[agent].transpose().mean().abs()\n",
    "    mean_shap_action_selected = mean_shap_action_selected.sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(y=mean_shap_action_selected, x=mean_shap_action_selected.index)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"|Mean SHAP values|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{result_path}/{agent}_mean_shap_action_selected.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "shap_dfs_action_nonselected = {}\n",
    "for agent in agents:\n",
    "    shap_summary = (np.mean(shap_values[agent], axis=0))\n",
    "\n",
    "    shap_summary_reduced = shap_summary[:-64]\n",
    "    feature_names_reduced = model_features[agent]\n",
    "    shap_df = pd.DataFrame(shap_summary_reduced, index=feature_names_reduced)\n",
    "\n",
    "    shap_df = shap_df.transpose().mean().abs().sort_values(ascending=False)    \n",
    "    \n",
    "    print(f\"Shape of shap_df for {agent}: {shap_df.shape}\")\n",
    "    shap_dfs_action_nonselected[agent] = shap_df\n",
    "\n",
    "name_mapping = {\n",
    "    'feed_flowrate'         : '1st Stage Feed\\nFlowrate',\n",
    "    '1st_permeate_flowrate' : '1st Stage Permeate\\nFlowrate',\n",
    "    '2nd_feed_flowrate'     : '2nd Stage Feed\\nFlowrate',\n",
    "    '2nd_permeate_flowrate' : '2nd Stage Permeate\\nFlowrate',\n",
    "    '2nd_brine_flowrate'    : '2nd Stage Concentrate\\nFlowrate',\n",
    "    'temperature'           : 'Temperature',\n",
    "    'feed_concentration'    : '1st Stage Feed\\nConcentration',\n",
    "    '1st_brine_concentration': '1st Stage Concentrate\\nConcentration',\n",
    "    '1st_brine_flowrate'    : '1st Stage Concentrate\\nFlowrate',\n",
    "    '1st_permeate_concentration': '1st Stage Permeate\\nConcentration',\n",
    "    '1st_brine_pressure'    : '1st Stage Concentrate\\nPressure',\n",
    "    '1st_pressure_applied'  : '1st Stage Pressure Applied',\n",
    "    '1st_recovery'          : '1st Stage Recovery',\n",
    "    '2nd_feed_concentration': '2nd Stage Feed\\nConcentration',\n",
    "    '2nd_brine_concentration': '2nd Stage Concentrate\\nConcentration',\n",
    "    '2nd_brine_flowrate'    : '2nd Stage Concentrate\\nFlowrate',\n",
    "    '2nd_permeate_concentration': '2nd Stage Permeate\\nConcentration',\n",
    "    '2nd_brine_pressure'    : '2nd Stage Concentrate\\nPressure',\n",
    "    '2nd_pressure_applied'  : '2nd Stage Pressure Applied'\n",
    "}\n",
    "\n",
    "for agent in agents:\n",
    "    shap_summary = (np.mean(shap_values[agent], axis=0))\n",
    "    shap_summary_reduced = shap_summary[:-64]\n",
    "    feature_names_reduced = model_features[agent]\n",
    "    shap_df = pd.DataFrame(shap_summary_reduced, index=feature_names_reduced)\n",
    "    \n",
    "    # Calculate mean and standard deviation of SHAP values across all actions\n",
    "    mean_shap_values = shap_df.transpose().mean()\n",
    "    std_shap_values = shap_df.transpose().std()\n",
    "    \n",
    "    # Sort by mean absolute SHAP value\n",
    "    mean_shap_values_abs = mean_shap_values.abs().sort_values(ascending=False).head(5)\n",
    "    mean_shap_values = mean_shap_values[mean_shap_values_abs.index]\n",
    "    std_shap_values = std_shap_values[mean_shap_values.index]  # Keep only top 5 features\n",
    "    \n",
    "    # Apply name mapping\n",
    "    mean_shap_values.index = [name_mapping.get(item, item) for item in mean_shap_values.index]\n",
    "    std_shap_values.index = mean_shap_values.index  # Ensure std has the same index\n",
    "    \n",
    "    plt.figure(figsize=(5, 7))\n",
    "    \n",
    "    # Define a colormap\n",
    "    cmap = mpl.colormaps.get_cmap('OrRd')\n",
    "    \n",
    "    # Normalize the absolute values to the range [0, 1]\n",
    "    norm = plt.Normalize(0, mean_shap_values.abs().max())\n",
    "    \n",
    "    # Create the bar plot with color mapping\n",
    "    colors = cmap(norm(mean_shap_values.abs()))\n",
    "    bars = sns.barplot(y=mean_shap_values, x=mean_shap_values.index, palette=colors)\n",
    "    \n",
    "    # Add error bars\n",
    "    plt.errorbar(x=mean_shap_values.index, y=mean_shap_values.values, yerr=std_shap_values.values,\n",
    "                 fmt=\"none\", color=\"black\", capsize=5)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    # plt.xlabel(\"Features\")\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel(\"Mean SHAP values\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{result_path}/{agent}_mean_shap_action_nonselected_with_errorbars_vertical.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for feature_idx, feature in enumerate(model_features['influent_flowrate']):\n",
    "    ax = axes[feature_idx]\n",
    "    for action_idx in range(shap_values['influent_flowrate'].shape[2]):\n",
    "        ax.scatter(observations['influent_flowrate'].detach().cpu().numpy()[:, feature_idx], shap_values['influent_flowrate'][:, feature_idx, action_idx], alpha=0.05, label=f'Action {action_idx}', marker='.', rasterized=True)\n",
    "    ax.set_xlabel('Observation')\n",
    "    ax.set_ylabel('SHAP Value')\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(len(model_features['influent_flowrate']), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{result_path}/SHAP_values_vs_observations-influent_flowrate.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for feature_idx, feature in enumerate(model_features['1st_stage_pump']):\n",
    "    ax = axes[feature_idx]\n",
    "    for action_idx in range(shap_values['1st_stage_pump'].shape[2]):\n",
    "        ax.scatter(observations['1st_stage_pump'].detach().cpu().numpy()[:, feature_idx], shap_values['1st_stage_pump'][:, feature_idx, action_idx], alpha=0.05, label=f'Action {action_idx}', marker='.', rasterized=True)\n",
    "    ax.set_xlabel('Observation')\n",
    "    ax.set_ylabel('SHAP Value')\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(len(model_features['1st_stage_pump']), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{result_path}/SHAP_values_vs_observations-1st_stage_pump.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for feature_idx, feature in enumerate(model_features['2nd_stage_pump']):\n",
    "    ax = axes[feature_idx]\n",
    "    for action_idx in range(shap_values['2nd_stage_pump'].shape[2]):\n",
    "        ax.scatter(observations['2nd_stage_pump'].detach().cpu().numpy()[:, feature_idx], shap_values['2nd_stage_pump'][:, feature_idx, action_idx], alpha=0.05, label=f'Action {action_idx}', marker='.', rasterized=True)\n",
    "    ax.set_xlabel('Observation')\n",
    "    ax.set_ylabel('SHAP Value')\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(len(model_features['2nd_stage_pump']), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{result_path}/SHAP_values_vs_observations-2nd_stage_pump.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    num_features = observations[agent].shape[1]\n",
    "    fig, axes = plt.subplots(nrows=num_features // 3 + (num_features % 3 > 0), ncols=3, figsize=(15, 5 * (num_features // 3 + (num_features % 3 > 0))))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for feature_idx in range(num_features):\n",
    "        ax = axes[feature_idx]\n",
    "        ax.hist(observations[agent].detach().cpu().numpy()[:, feature_idx], bins=50, alpha=0.7)\n",
    "        ax.set_xlabel('Observation Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Apply name mapping to the title\n",
    "        feature_name = model_features[agent][feature_idx]\n",
    "        mapped_feature_name = name_mapping.get(feature_name, feature_name)\n",
    "        ax.set_title(f'{mapped_feature_name}')\n",
    "\n",
    "    # Remove empty subplots\n",
    "    for i in range(num_features, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Observation Histograms for {agent}', fontsize=16, y=1.02)\n",
    "    plt.savefig(f'{result_path}/observation_histograms_{agent}.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    obs = observations[agent].detach().cpu().numpy()\n",
    "    df = pd.DataFrame(obs, columns=model_features[agent])\n",
    "    sns.pairplot(df, markers='.', plot_kws={'alpha': 0.5})\n",
    "    plt.suptitle(f'Pairplot of Observations for {agent}', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Apply name mapping to the plot labels\n",
    "    for ax in plt.gcf().axes:\n",
    "        if ax.get_xlabel() in name_mapping:\n",
    "            ax.set_xlabel(name_mapping[ax.get_xlabel()])\n",
    "        if ax.get_ylabel() in name_mapping:\n",
    "            ax.set_ylabel(name_mapping[ax.get_ylabel()])\n",
    "    plt.savefig(f'{result_path}/pairplot_observations_{agent}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name_map = {'influent_flowrate': 'Feed Flow Rate Controller', '1st_stage_pump': 'HPP Pressure Controller', '2nd_stage_pump': 'IBP Pressure Controller'}\n",
    "\n",
    "for agent in agents:\n",
    "    obs = observations[agent].detach().cpu().numpy()\n",
    "    df = pd.DataFrame(obs, columns=model_features[agent])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr = df.corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle\n",
    "    # mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    mask = np.zeros_like(corr, dtype=bool)\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    # cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    # Annotate each cell with the correlation value\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            if not mask[i, j]:\n",
    "                ax.text(j + 0.5, i + 0.5, f\"{corr.iloc[i, j]:.2f}\",\n",
    "                        ha=\"center\", va=\"center\", color=\"black\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    # plt.yticks(rotation=45, ha='right')\n",
    "    ax.set_xticklabels([name_mapping.get(item.get_text(), item.get_text()) for item in ax.get_xticklabels()], fontsize=10)\n",
    "    ax.set_yticklabels([name_mapping.get(item.get_text(), item.get_text()) for item in ax.get_yticklabels()], fontsize=10)\n",
    "    plt.title(f'Correlation Heatmap of Observations for {agent_name_map[agent]}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_path}/correlation_heatmap_observations_{agent}.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROMARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
